{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bdb0a43",
   "metadata": {},
   "source": [
    "# Agent Search with Amazon OpenSearch\n",
    "\n",
    "In this module, we'll build an intelligent AI agent that can understand natural language queries and interact with our OpenSearch semantic search system. This represents the next evolution in search technology - from static keyword searches to conversational, context-aware search experiences.\n",
    "\n",
    "## What We'll Build\n",
    "\n",
    "1. **Conversational Search Agent**: An AI agent that can understand natural language queries and convert them into effective search strategies\n",
    "2. **Context-Aware Responses**: The agent maintains conversation context and can handle follow-up questions\n",
    "3. **Multi-Modal Search**: Combine keyword, semantic, and contextual search based on query analysis\n",
    "4. **Intelligent Result Synthesis**: Use LLMs to synthesize search results into coherent, helpful responses\n",
    "5. **Interactive Demo**: A working demonstration of agent-powered search\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "Our agent search system combines:\n",
    "- **Language Model**: AWS Bedrock (Claude/Titan) for natural language understanding and response generation\n",
    "- **Search Engine**: OpenSearch with semantic and keyword search capabilities\n",
    "- **Agent Framework**: Custom agent logic to orchestrate search and response generation\n",
    "- **Context Management**: Conversation history and context tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e02e76",
   "metadata": {},
   "source": [
    "### 1. Install Required Libraries\n",
    "\n",
    "We'll need additional libraries for building our AI agent, including AWS Bedrock SDK and conversation management tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8348dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q boto3\n",
    "!pip install -q requests\n",
    "!pip install -q requests-aws4auth\n",
    "!pip install -q opensearch-py\n",
    "!pip install -q sentence-transformers\n",
    "!pip install -q pandas\n",
    "!pip install -q numpy\n",
    "!pip install -q tqdm\n",
    "# Install additional libraries for agent functionality\n",
    "!pip install -q langchain\n",
    "!pip install -q langchain-aws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_imports",
   "metadata": {},
   "source": [
    "### 2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "import time\n",
    "\n",
    "# OpenSearch and search libraries\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "# Sentence transformers for embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# AWS Bedrock for LLM capabilities\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "print(\"‚úÖ All required libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_config",
   "metadata": {},
   "source": [
    "### 3. Setup Configuration and CloudFormation Outputs\n",
    "\n",
    "Let's get the configuration from our CloudFormation stack that was set up in previous modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_cfn",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def get_cfn_outputs(stackname):\n",
    "    \"\"\"Get CloudFormation stack outputs\"\"\"\n",
    "    cfn = boto3.client('cloudformation')\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "# Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"static-cloudformation-semantic-search\"\n",
    "\n",
    "try:\n",
    "    outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "    bucket = outputs['s3BucketTraining']\n",
    "    aos_host = outputs['OpenSearchDomainEndpoint']\n",
    "    \n",
    "    print(\"‚úÖ CloudFormation outputs retrieved successfully!\")\n",
    "    print(f\"S3 Bucket: {bucket}\")\n",
    "    print(f\"OpenSearch Host: {aos_host}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not retrieve CloudFormation outputs: {e}\")\n",
    "    print(\"Please ensure your CloudFormation stack is deployed and accessible.\")\n",
    "    # Fallback configuration\n",
    "    aos_host = \"your-opensearch-domain.us-east-1.es.amazonaws.com\"\n",
    "    bucket = \"your-s3-bucket\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_connections",
   "metadata": {},
   "source": [
    "### 4. Setup AWS Service Connections\n",
    "\n",
    "We'll establish connections to OpenSearch, Bedrock, and other AWS services needed for our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_aws",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup AWS region and credentials\n",
    "region = 'us-east-1'\n",
    "\n",
    "# Setup OpenSearch connection\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region)\n",
    "\n",
    "aos_client = OpenSearch(\n",
    "    hosts=[{'host': aos_host, 'port': 443}],\n",
    "    http_auth=auth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection\n",
    ")\n",
    "\n",
    "# Setup Bedrock client for LLM capabilities\n",
    "bedrock_client = boto3.client('bedrock-runtime', region_name=region)\n",
    "\n",
    "# Initialize sentence transformer for generating embeddings\n",
    "try:\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    print(\"‚úÖ Sentence transformer model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load sentence transformer: {e}\")\n",
    "    print(\"Will download model on first use.\")\n",
    "\n",
    "print(\"‚úÖ AWS service connections established!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent_class",
   "metadata": {},
   "source": [
    "### 5. Build the Search Agent Class\n",
    "\n",
    "Now we'll create our intelligent search agent that combines natural language understanding with OpenSearch capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "search_agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchAgent:\n",
    "    \"\"\"Intelligent Search Agent for OpenSearch with conversational capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, opensearch_client, bedrock_client, sentence_model, index_name='nlp_pqa'):\n",
    "        self.os_client = opensearch_client\n",
    "        self.bedrock_client = bedrock_client\n",
    "        self.sentence_model = sentence_model\n",
    "        self.index_name = index_name\n",
    "        self.conversation_history = []\n",
    "        \n",
    "    def analyze_query_intent(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze user query to determine search strategy\"\"\"\n",
    "        \n",
    "        # Simple heuristic-based intent analysis\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        intent_analysis = {\n",
    "            'query': query,\n",
    "            'search_type': 'semantic',  # Default to semantic search\n",
    "            'needs_context': False,\n",
    "            'is_followup': False,\n",
    "            'extracted_keywords': [],\n",
    "            'confidence': 0.8\n",
    "        }\n",
    "        \n",
    "        # Check for follow-up patterns\n",
    "        followup_patterns = ['what about', 'how about', 'what if', 'can you', 'do you', 'more like']\n",
    "        if any(pattern in query_lower for pattern in followup_patterns) and self.conversation_history:\n",
    "            intent_analysis['is_followup'] = True\n",
    "            intent_analysis['needs_context'] = True\n",
    "        \n",
    "        # Check for keyword search indicators\n",
    "        keyword_indicators = ['exact', 'specifically', 'precisely', 'literally']\n",
    "        if any(indicator in query_lower for indicator in keyword_indicators):\n",
    "            intent_analysis['search_type'] = 'keyword'\n",
    "        \n",
    "        # Check for comparison queries\n",
    "        comparison_patterns = ['compare', 'difference', 'versus', 'vs', 'better']\n",
    "        if any(pattern in query_lower for pattern in comparison_patterns):\n",
    "            intent_analysis['search_type'] = 'hybrid'\n",
    "        \n",
    "        return intent_analysis\n",
    "    \n",
    "    def perform_semantic_search(self, query: str, k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Perform semantic search using vector embeddings\"\"\"\n",
    "        try:\n",
    "            # Generate embedding for the query\n",
    "            query_vector = self.sentence_model.encode([query])[0].tolist()\n",
    "            \n",
    "            # Perform KNN search\n",
    "            search_body = {\n",
    "                'size': k,\n",
    "                'query': {\n",
    "                    'knn': {\n",
    "                        'question_vector': {\n",
    "                            'vector': query_vector,\n",
    "                            'k': k\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                '_source': ['question', 'answer']\n",
    "            }\n",
    "            \n",
    "            response = self.os_client.search(index=self.index_name, body=search_body)\n",
    "            \n",
    "            results = []\n",
    "            for hit in response['hits']['hits']:\n",
    "                results.append({\n",
    "                    'score': hit['_score'],\n",
    "                    'question': hit['_source']['question'],\n",
    "                    'answer': hit['_source']['answer'],\n",
    "                    'search_type': 'semantic'\n",
    "                })\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in semantic search: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def perform_keyword_search(self, query: str, k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Perform traditional keyword search\"\"\"\n",
    "        try:\n",
    "            search_body = {\n",
    "                'size': k,\n",
    "                'query': {\n",
    "                    'match': {\n",
    "                        'question': {\n",
    "                            'query': query,\n",
    "                            'operator': 'or'\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                '_source': ['question', 'answer']\n",
    "            }\n",
    "            \n",
    "            response = self.os_client.search(index=self.index_name, body=search_body)\n",
    "            \n",
    "            results = []\n",
    "            for hit in response['hits']['hits']:\n",
    "                results.append({\n",
    "                    'score': hit['_score'],\n",
    "                    'question': hit['_source']['question'],\n",
    "                    'answer': hit['_source']['answer'],\n",
    "                    'search_type': 'keyword'\n",
    "                })\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in keyword search: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def perform_hybrid_search(self, query: str, k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Combine semantic and keyword search results\"\"\"\n",
    "        semantic_results = self.perform_semantic_search(query, k//2)\n",
    "        keyword_results = self.perform_keyword_search(query, k//2)\n",
    "        \n",
    "        # Combine and deduplicate results\n",
    "        all_results = semantic_results + keyword_results\n",
    "        \n",
    "        # Simple deduplication based on question text\n",
    "        seen_questions = set()\n",
    "        unique_results = []\n",
    "        \n",
    "        for result in all_results:\n",
    "            if result['question'] not in seen_questions:\n",
    "                seen_questions.add(result['question'])\n",
    "                result['search_type'] = 'hybrid'\n",
    "                unique_results.append(result)\n",
    "        \n",
    "        return unique_results[:k]\n",
    "    \n",
    "    def generate_llm_response(self, query: str, search_results: List[Dict], intent_analysis: Dict) -> str:\n",
    "        \"\"\"Generate natural language response using LLM\"\"\"\n",
    "        \n",
    "        # Prepare context from search results\n",
    "        context_parts = []\n",
    "        for i, result in enumerate(search_results[:5], 1):\n",
    "            context_parts.append(f\"Result {i}:\\nQ: {result['question']}\\nA: {result['answer']}\\n\")\n",
    "        \n",
    "        context = \"\\n\".join(context_parts)\n",
    "        \n",
    "        # Add conversation history if this is a follow-up\n",
    "        history_context = \"\"\n",
    "        if intent_analysis['is_followup'] and self.conversation_history:\n",
    "            recent_history = self.conversation_history[-2:]\n",
    "            history_context = \"\\n\\nPrevious conversation:\\n\"\n",
    "            for hist in recent_history:\n",
    "                history_context += f\"User: {hist['query']}\\nAssistant: {hist['response'][:200]}...\\n\\n\"\n",
    "        \n",
    "        try:\n",
    "            # Use a simple fallback response generation for demo purposes\n",
    "            # In a real implementation, you would call AWS Bedrock here\n",
    "            \n",
    "            if search_results:\n",
    "                top_result = search_results[0]\n",
    "                response = f\"Based on the search results, here's what I found:\\n\\n\"\n",
    "                response += f\"**Most Relevant Result:**\\n\"\n",
    "                response += f\"Q: {top_result['question']}\\n\"\n",
    "                response += f\"A: {top_result['answer']}\\n\\n\"\n",
    "                \n",
    "                if len(search_results) > 1:\n",
    "                    response += f\"I found {len(search_results)} related results. \"\n",
    "                    if intent_analysis['search_type'] == 'semantic':\n",
    "                        response += \"These were found using semantic search to understand the meaning of your question.\"\n",
    "                    elif intent_analysis['search_type'] == 'keyword':\n",
    "                        response += \"These were found using keyword matching.\"\n",
    "                    else:\n",
    "                        response += \"These were found using a combination of semantic and keyword search.\"\n",
    "                \n",
    "                return response\n",
    "            else:\n",
    "                return \"I couldn't find specific results for your query. Could you try rephrasing your question or asking about a different topic?\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating LLM response: {e}\")\n",
    "            # Fallback to simple response\n",
    "            if search_results:\n",
    "                return f\"I found {len(search_results)} results for your query. The top result is: {search_results[0]['answer'][:200]}...\"\n",
    "            else:\n",
    "                return \"I couldn't find specific results for your query.\"\n",
    "    \n",
    "    def process_query(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Main method to process a user query and return agent response\"\"\"\n",
    "        \n",
    "        print(f\"üîç Processing query: '{query}'\")\n",
    "        \n",
    "        # Step 1: Analyze query intent\n",
    "        intent_analysis = self.analyze_query_intent(query)\n",
    "        print(f\"   Intent Analysis: {intent_analysis['search_type']} search, followup: {intent_analysis['is_followup']}\")\n",
    "        \n",
    "        # Step 2: Perform appropriate search\n",
    "        if intent_analysis['search_type'] == 'semantic':\n",
    "            search_results = self.perform_semantic_search(query)\n",
    "        elif intent_analysis['search_type'] == 'keyword':\n",
    "            search_results = self.perform_keyword_search(query)\n",
    "        else:  # hybrid\n",
    "            search_results = self.perform_hybrid_search(query)\n",
    "        \n",
    "        print(f\"   Found {len(search_results)} search results\")\n",
    "        \n",
    "        # Step 3: Generate intelligent response\n",
    "        response = self.generate_llm_response(query, search_results, intent_analysis)\n",
    "        \n",
    "        # Step 4: Store conversation history\n",
    "        conversation_entry = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'query': query,\n",
    "            'intent_analysis': intent_analysis,\n",
    "            'search_results_count': len(search_results),\n",
    "            'response': response\n",
    "        }\n",
    "        \n",
    "        self.conversation_history.append(conversation_entry)\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'response': response,\n",
    "            'intent_analysis': intent_analysis,\n",
    "            'search_results': search_results[:3],  # Return top 3 for reference\n",
    "            'timestamp': conversation_entry['timestamp']\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ SearchAgent class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initialize_agent",
   "metadata": {},
   "source": [
    "### 6. Initialize the Search Agent\n",
    "\n",
    "Now let's create an instance of our search agent and verify it can connect to OpenSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init_agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the search agent\n",
    "try:\n",
    "    agent = SearchAgent(\n",
    "        opensearch_client=aos_client,\n",
    "        bedrock_client=bedrock_client,\n",
    "        sentence_model=model,\n",
    "        index_name='nlp_pqa'\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Search Agent initialized successfully!\")\n",
    "    \n",
    "    # Test OpenSearch connection\n",
    "    try:\n",
    "        index_info = aos_client.indices.get(index='nlp_pqa')\n",
    "        print(f\"‚úÖ Connected to OpenSearch index 'nlp_pqa'\")\n",
    "        \n",
    "        # Get document count\n",
    "        count_response = aos_client.count(index='nlp_pqa')\n",
    "        doc_count = count_response['count']\n",
    "        print(f\"   Index contains {doc_count} documents\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not access OpenSearch index: {e}\")\n",
    "        print(\"Please ensure you have completed Module 3 to set up the semantic search index.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing Search Agent: {e}\")\n",
    "    print(\"Please check your AWS credentials and OpenSearch configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo_queries",
   "metadata": {},
   "source": [
    "### 7. Interactive Agent Demo\n",
    "\n",
    "Let's demonstrate our agent's capabilities with various types of queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo_basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 1: Basic Product Question\n",
    "print(\"=== Demo 1: Basic Product Question ===\")\n",
    "result1 = agent.process_query(\"Does this work with Xbox?\")\n",
    "\n",
    "print(f\"\\nü§ñ Agent Response:\")\n",
    "print(result1['response'])\n",
    "print(f\"\\nüìä Search Strategy: {result1['intent_analysis']['search_type']}\")\n",
    "print(f\"üìà Results Found: {len(result1['search_results'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo_followup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 2: Follow-up Question (tests context awareness)\n",
    "print(\"\\n=== Demo 2: Follow-up Question ===\")\n",
    "result2 = agent.process_query(\"What about PlayStation?\")\n",
    "\n",
    "print(f\"\\nü§ñ Agent Response:\")\n",
    "print(result2['response'])\n",
    "print(f\"\\nüìä Search Strategy: {result2['intent_analysis']['search_type']}\")\n",
    "print(f\"üìà Results Found: {len(result2['search_results'])}\")\n",
    "print(f\"üîó Is Follow-up: {result2['intent_analysis']['is_followup']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 3: Comparison Query (triggers hybrid search)\n",
    "print(\"\\n=== Demo 3: Comparison Query ===\")\n",
    "result3 = agent.process_query(\"Compare wireless vs wired headphones for gaming\")\n",
    "\n",
    "print(f\"\\nü§ñ Agent Response:\")\n",
    "print(result3['response'])\n",
    "print(f\"\\nüìä Search Strategy: {result3['intent_analysis']['search_type']}\")\n",
    "print(f\"üìà Results Found: {len(result3['search_results'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo_specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 4: Specific Technical Query\n",
    "print(\"\\n=== Demo 4: Technical Question ===\")\n",
    "result4 = agent.process_query(\"How do I connect Bluetooth headphones to my device?\")\n",
    "\n",
    "print(f\"\\nü§ñ Agent Response:\")\n",
    "print(result4['response'])\n",
    "print(f\"\\nüìä Search Strategy: {result4['intent_analysis']['search_type']}\")\n",
    "print(f\"üìà Results Found: {len(result4['search_results'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conversation_history",
   "metadata": {},
   "source": [
    "### 8. View Conversation History\n",
    "\n",
    "Let's examine how our agent maintains conversation context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view_history",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Conversation History ===\")\n",
    "print(f\"Total interactions: {len(agent.conversation_history)}\\n\")\n",
    "\n",
    "for i, entry in enumerate(agent.conversation_history, 1):\n",
    "    print(f\"Interaction {i}:\")\n",
    "    print(f\"  üïê Time: {entry['timestamp']}\")\n",
    "    print(f\"  üë§ User: {entry['query']}\")\n",
    "    print(f\"  üîç Search: {entry['intent_analysis']['search_type']}\")\n",
    "    print(f\"  üìä Results: {entry['search_results_count']}\")\n",
    "    print(f\"  üîó Follow-up: {entry['intent_analysis']['is_followup']}\")\n",
    "    print(f\"  ü§ñ Response: {entry['response'][:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interactive_demo",
   "metadata": {},
   "source": [
    "### 9. Interactive Agent Session\n",
    "\n",
    "Now you can interact with the agent directly! Run the cell below and ask your own questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interactive_session",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_agent_session():\n",
    "    \"\"\"Interactive session with the search agent\"\"\"\n",
    "    print(\"ü§ñ Welcome to the Interactive Search Agent!\")\n",
    "    print(\"Type your questions about products, or 'quit' to exit.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_query = input(\"üë§ You: \").strip()\n",
    "            \n",
    "            if user_query.lower() in ['quit', 'exit', 'bye']:\n",
    "                print(\"ü§ñ Agent: Goodbye! Thanks for using the search agent.\")\n",
    "                break\n",
    "            \n",
    "            if not user_query:\n",
    "                print(\"ü§ñ Agent: Please enter a question about products.\")\n",
    "                continue\n",
    "            \n",
    "            # Process the query\n",
    "            result = agent.process_query(user_query)\n",
    "            \n",
    "            print(f\"\\nü§ñ Agent: {result['response']}\")\n",
    "            print(f\"\\nüìä (Used {result['intent_analysis']['search_type']} search with {len(result['search_results'])} results)\\n\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nü§ñ Agent: Session ended by user.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"ü§ñ Agent: Sorry, I encountered an error: {e}\")\n",
    "\n",
    "# Uncomment the line below to start an interactive session\n",
    "# interactive_agent_session()\n",
    "\n",
    "print(\"üí° Tip: Uncomment the line above to start an interactive session with the agent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance_analysis",
   "metadata": {},
   "source": [
    "### 10. Performance Analysis\n",
    "\n",
    "Let's analyze how different search strategies perform for various query types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries to evaluate different search strategies\n",
    "test_queries = [\n",
    "    \"Does this work with Xbox?\",\n",
    "    \"wireless headphones battery life\",\n",
    "    \"How to connect Bluetooth?\",\n",
    "    \"gaming headset microphone quality\",\n",
    "    \"compatible with iPhone\",\n",
    "    \"noise cancellation effectiveness\",\n",
    "    \"What's the sound quality like?\",\n",
    "    \"charging time for wireless earbuds\"\n",
    "]\n",
    "\n",
    "print(\"=== Performance Analysis ===\")\n",
    "print(\"Testing different search strategies on sample queries...\\n\")\n",
    "\n",
    "performance_data = []\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"Testing: '{query}'\")\n",
    "    \n",
    "    # Test semantic search\n",
    "    start_time = time.time()\n",
    "    semantic_results = agent.perform_semantic_search(query, k=5)\n",
    "    semantic_time = time.time() - start_time\n",
    "    \n",
    "    # Test keyword search\n",
    "    start_time = time.time()\n",
    "    keyword_results = agent.perform_keyword_search(query, k=5)\n",
    "    keyword_time = time.time() - start_time\n",
    "    \n",
    "    # Test hybrid search\n",
    "    start_time = time.time()\n",
    "    hybrid_results = agent.perform_hybrid_search(query, k=5)\n",
    "    hybrid_time = time.time() - start_time\n",
    "    \n",
    "    performance_data.append({\n",
    "        'query': query,\n",
    "        'semantic_results': len(semantic_results),\n",
    "        'semantic_time': semantic_time,\n",
    "        'keyword_results': len(keyword_results),\n",
    "        'keyword_time': keyword_time,\n",
    "        'hybrid_results': len(hybrid_results),\n",
    "        'hybrid_time': hybrid_time\n",
    "    })\n",
    "    \n",
    "    print(f\"  Semantic: {len(semantic_results)} results in {semantic_time:.3f}s\")\n",
    "    print(f\"  Keyword:  {len(keyword_results)} results in {keyword_time:.3f}s\")\n",
    "    print(f\"  Hybrid:   {len(hybrid_results)} results in {hybrid_time:.3f}s\\n\")\n",
    "\n",
    "# Create performance summary\n",
    "df_performance = pd.DataFrame(performance_data)\n",
    "print(\"=== Performance Summary ===\")\n",
    "print(f\"Average Results per Search Type:\")\n",
    "print(f\"  Semantic: {df_performance['semantic_results'].mean():.1f} results\")\n",
    "print(f\"  Keyword:  {df_performance['keyword_results'].mean():.1f} results\")\n",
    "print(f\"  Hybrid:   {df_performance['hybrid_results'].mean():.1f} results\")\n",
    "\n",
    "print(f\"\\nAverage Response Time:\")\n",
    "print(f\"  Semantic: {df_performance['semantic_time'].mean():.3f}s\")\n",
    "print(f\"  Keyword:  {df_performance['keyword_time'].mean():.3f}s\")\n",
    "print(f\"  Hybrid:   {df_performance['hybrid_time'].mean():.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "### 11. Summary and Next Steps\n",
    "\n",
    "Congratulations! You've successfully built and demonstrated an intelligent search agent with OpenSearch. Let's summarize what we've accomplished and explore next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ Module 6 - Agent Search with OpenSearch Complete!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "print(\"\\nüìã What We Built:\")\n",
    "print(\"   ‚úÖ Intelligent Search Agent with natural language understanding\")\n",
    "print(\"   ‚úÖ Multi-strategy search (semantic, keyword, hybrid)\")\n",
    "print(\"   ‚úÖ Conversation context and follow-up handling\")\n",
    "print(\"   ‚úÖ Query intent analysis and routing\")\n",
    "print(\"   ‚úÖ Interactive demonstration interface\")\n",
    "print(\"   ‚úÖ Performance analysis and insights\")\n",
    "\n",
    "print(\"\\nüîß Key Technologies Used:\")\n",
    "print(\"   ‚Ä¢ Amazon OpenSearch Service for vector and text search\")\n",
    "print(\"   ‚Ä¢ Sentence Transformers for embedding generation\")\n",
    "print(\"   ‚Ä¢ AWS Bedrock (framework ready) for LLM responses\")\n",
    "print(\"   ‚Ä¢ Custom agent logic for intelligent query routing\")\n",
    "print(\"   ‚Ä¢ Conversation history and context management\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps and Enhancements:\")\n",
    "print(\"   1. Integrate AWS Bedrock for production LLM responses\")\n",
    "print(\"   2. Add voice interface using Amazon Polly/Transcribe\")\n",
    "print(\"   3. Implement user authentication and personalization\")\n",
    "print(\"   4. Add product recommendation capabilities\")\n",
    "print(\"   5. Create a web interface for the agent\")\n",
    "print(\"   6. Implement feedback learning and model fine-tuning\")\n",
    "print(\"   7. Add multi-modal search (images, audio)\")\n",
    "print(\"   8. Scale to handle multiple users concurrently\")\n",
    "\n",
    "print(\"\\nüí° Real-World Applications:\")\n",
    "print(\"   ‚Ä¢ Customer service chatbots\")\n",
    "print(\"   ‚Ä¢ E-commerce product discovery\")\n",
    "print(\"   ‚Ä¢ Knowledge base search\")\n",
    "print(\"   ‚Ä¢ Technical documentation assistance\")\n",
    "print(\"   ‚Ä¢ Educational content discovery\")\n",
    "\n",
    "print(\"\\nüéØ Key Learning Outcomes:\")\n",
    "print(\"   ‚Ä¢ How to build conversational search interfaces\")\n",
    "print(\"   ‚Ä¢ Combining multiple search strategies intelligently\")\n",
    "print(\"   ‚Ä¢ Implementing context-aware AI agents\")\n",
    "print(\"   ‚Ä¢ Performance optimization for real-time search\")\n",
    "print(\"   ‚Ä¢ User experience design for AI-powered applications\")\n",
    "\n",
    "if agent.conversation_history:\n",
    "    print(f\"\\nüìä Your Session Stats:\")\n",
    "    print(f\"   ‚Ä¢ Total queries processed: {len(agent.conversation_history)}\")\n",
    "    followup_count = sum(1 for entry in agent.conversation_history if entry['intent_analysis']['is_followup'])\n",
    "    print(f\"   ‚Ä¢ Follow-up queries: {followup_count}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Thank you for completing the Agent Search tutorial!\")\n",
    "print(\"You now have the foundation to build intelligent, conversational search experiences.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}