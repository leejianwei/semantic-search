{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0cc63eb",
   "metadata": {},
   "source": [
    "# Module 1 : Exploring BM25 similiarity and Semantic similiarity\n",
    "\n",
    "Before we get started with Amazon OpenSearch and our search web app, let's explore some of the core concepts in search. Below, we'll demonstrate the different between algorithms for matching data using BM25 similarity (keyword matching) and Cosine similarity (sematnic vector matching)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426b572b",
   "metadata": {},
   "source": [
    "### 1. Upgrade PyTorch and restart Kernel\n",
    "\n",
    "Before we begin, we need to upgrade PyTorch and restart the notebook kernel. The following should take 2-3 minutes to complete, and you should see the following message::\"Successfully intalled torch-1.nn.n\".\n",
    "\n",
    "You may see a message with stating \"ERROR: pip's dependency resolver does not...\" - you can ignore this error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a620610",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb43e7eb",
   "metadata": {},
   "source": [
    "Now we need to restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36b603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "restartkernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a67de4",
   "metadata": {},
   "source": [
    "Next, let's verify the version of Torch to ensure everything is up to date. The version should be 1.10.2 or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e84e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4421e568",
   "metadata": {},
   "source": [
    "### 2. Install Pre-Requisites\n",
    "\n",
    "Before we can experiment with different searches, we need to install some required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fde2524",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers\n",
    "!pip install -U sentence-transformers rank_bm25\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import string\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42fd870",
   "metadata": {},
   "source": [
    "### 3. Create a sample dataset\n",
    "\n",
    "Let's now create a very simple dataset as an array of 4 questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaf7a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "passages=[\"does this work with xbox?\",\n",
    "          \"Does the M70 work with Android phones?\", \n",
    "          \"does this work with iphone?\",\n",
    "          \"Can this work with an xbox \"\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f381a",
   "metadata": {},
   "source": [
    "### 4. Explore BM25 similiarity \n",
    "\n",
    "Execute the following to explore BM25 similarity. First, we'll tokenize the data set, then use BM25 similarity to compare the phrase \"does this work with xbox?\" with our sample questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7fc7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_tokenizer(text):\n",
    "    tokenized_doc = []\n",
    "    for token in text.lower().split():\n",
    "        token = token.strip(string.punctuation)\n",
    "\n",
    "        if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n",
    "            tokenized_doc.append(token)\n",
    "    return tokenized_doc\n",
    "\n",
    "\n",
    "tokenized_corpus = []\n",
    "for passage in tqdm(passages):\n",
    "    tokenized_corpus.append(bm25_tokenizer(passage))\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "bm25_scores = bm25.get_scores(bm25_tokenizer(\"does this work with xbox?\"))\n",
    "\n",
    "all_sentence_combinations = []\n",
    "for i in range(len(bm25_scores)):\n",
    "    all_sentence_combinations.append([bm25_scores[i], i])\n",
    "\n",
    "all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print(\"Top most similar pairs:\")\n",
    "for score, i in all_sentence_combinations[0:4]:\n",
    "    print(\"{} \\t {} \\t {:.4f}\".format(passages[i],bm25_tokenizer(passages[i]),bm25_scores[i]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2389c01",
   "metadata": {},
   "source": [
    "### 5. Semantic Similiarities\n",
    "\n",
    "\n",
    "Execute the following to explore semantic similarity with cosine similarity. In this code, we'll use the same dataset as above, but using cosine similarity. Compare the differences in how matches are ranked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec61c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#Encode all sentences\n",
    "embeddings = model.encode(passages)\n",
    "\n",
    "#Compute cosine similarity between all pairs\n",
    "cos_sim = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "#cosine similarity score with query\n",
    "all_sentence_combinations = []\n",
    "for i in range(len(cos_sim)):\n",
    "    all_sentence_combinations.append([cos_sim[0][i], i])\n",
    "\n",
    "#Sort list by the highest cosine similarity score\n",
    "all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print(\"Top most similar pairs:\")\n",
    "for score, i in all_sentence_combinations[0:4]:\n",
    "    print(\"{} \\t {:.4f}\".format(passages[i],cos_sim[0][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47a7641",
   "metadata": {},
   "source": [
    "### 6. Compare the differences.\n",
    "\n",
    "As you can see, the similarity is significantly different, even with with a trivial data set.\n",
    "\n",
    "In this module, we've used fairly simple steps with a very small dataset to demonstrate the difference between BM25 and cosine similarity. In the following modules, we'll demonstrate these same concepts with using OpenSearch and a larger and more complex dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
