{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2a46493",
   "metadata": {},
   "source": [
    "# Semantic Search with Fine Tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b17aaaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (1.7.1)\n",
      "Collecting torch\n",
      "  Using cached torch-1.10.2-cp36-cp36m-manylinux1_x86_64.whl (881.9 MB)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torch) (4.0.1)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torch) (0.8)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.7.1\n",
      "    Uninstalling torch-1.7.1:\n",
      "      Successfully uninstalled torch-1.7.1\n",
      "Successfully installed torch-1.10.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9da096e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display_html\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "restartkernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d463f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.2+cu102\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7f438ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
      "Collecting rank_bm25\n",
      "  Using cached rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sentence-transformers) (0.8.2)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sentence-transformers) (0.24.1)\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Using cached huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sentence-transformers) (4.62.3)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sentence-transformers) (1.5.3)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sentence-transformers) (1.10.2)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Using cached transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sentence-transformers) (1.19.2)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sentence-transformers) (3.4.4)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.97-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.0.1)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.4.1-py3-none-any.whl (9.9 kB)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.26.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.7.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (5.4.1)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torch>=1.6.0->sentence-transformers) (0.8)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2020.11.13)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.53-py3-none-any.whl\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Using cached tokenizers-0.12.1-cp36-cp36m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from nltk->sentence-transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from scikit-learn->sentence-transformers) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from scikit-learn->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torchvision->sentence-transformers) (8.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.9)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
      "Installing collected packages: filelock, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece, sentence-transformers, rank-bm25\n",
      "Successfully installed filelock-3.4.1 huggingface-hub-0.4.0 rank-bm25-0.2.2 sacremoses-0.0.53 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.12.1 transformers-4.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers rank_bm25\n",
    "!pip install -q opensearch-py\n",
    "!pip install -q tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce49d9dd",
   "metadata": {},
   "source": [
    "### Note change \"cloudformation_stack_name\" to the Cloud Formation stack name when you provision your env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3575d7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'DomainEndpoint': 'search-opensearchservi-syxjz3qrneyt-qwv7yjocaeblepoky43ienflvu.us-east-1.es.amazonaws.com',\n",
       " 'S3BucketSecureURL': 'https://static-cloudformation-semantic-se-s3buckethosting-18ofta7sitf9g.s3.amazonaws.com',\n",
       " 'SageMakerNotebookURL': 'https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/notebook-instances/openNotebook/NotebookInstance-FayBdMH70xG5?view=classic',\n",
       " 'osArn': 'arn:aws:es:us-east-1:522880334446:domain/opensearchservi-syxjz3qrneyt',\n",
       " 's3BucketTraining': 'static-cloudformation-semantic-s-s3buckettraining-5wyh4fklf11q',\n",
       " 'osDomainName': 'opensearchservi-syxjz3qrneyt',\n",
       " 's3BucketHostingBucketName': 'static-cloudformation-semantic-se-s3buckethosting-18ofta7sitf9g'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"static-cloudformation-semantic-search\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "\n",
    "bucket = outputs['s3BucketTraining']\n",
    "aos_host = outputs['DomainEndpoint']\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f0f3c8",
   "metadata": {},
   "source": [
    "## Step 1: Fine Tune the modal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8944fe68",
   "metadata": {},
   "source": [
    "## Fine Tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3821faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import SentenceTransformer,  LoggingHandler, losses, models, util\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.readers import InputExample\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import gzip\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5b1a1b",
   "metadata": {},
   "source": [
    "Load data set of Amazon Product Question and Answer data from : https://registry.opendata.aws/amazon-pqa/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "458d6919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-20 13:11:25 2267692311 amazon-pqa.tar.gz\r\n",
      "2021-05-09 11:53:53  442066567 amazon_pqa_accessories.json\r\n",
      "2021-05-09 11:53:49  275062405 amazon_pqa_activity_&_fitness_trackers.json\r\n",
      "2021-05-09 11:53:49  127094083 amazon_pqa_adapters.json\r\n",
      "2021-05-09 11:53:49  143639699 amazon_pqa_amazon_echo_&_alexa_devices.json\r\n",
      "2021-05-09 11:53:49  106017252 amazon_pqa_area_rugs.json\r\n",
      "2021-05-09 11:53:49  164430689 amazon_pqa_backpacks.json\r\n",
      "2021-05-09 11:53:49  679285046 amazon_pqa_basic_cases.json\r\n",
      "2021-05-09 11:53:49  390964941 amazon_pqa_batteries.json\r\n",
      "2021-05-09 11:53:49  107896488 amazon_pqa_battery_chargers.json\r\n",
      "2021-05-09 11:53:49   77113272 amazon_pqa_bed_frames.json\r\n",
      "2021-05-09 11:53:49  157944761 amazon_pqa_beds.json\r\n",
      "2021-05-09 11:53:49  218133567 amazon_pqa_bullet_cameras.json\r\n",
      "2021-05-09 11:53:50  118106256 amazon_pqa_camcorders.json\r\n",
      "2021-05-09 11:53:50   71239417 amazon_pqa_car.json\r\n",
      "2021-05-09 11:53:50  137487049 amazon_pqa_car_stereo_receivers.json\r\n",
      "2021-05-09 11:53:50  153301436 amazon_pqa_carrier_cell_phones.json\r\n",
      "2021-05-09 11:53:50  331996893 amazon_pqa_cases.json\r\n",
      "2021-05-09 11:53:50  194789327 amazon_pqa_casual.json\r\n",
      "2021-05-09 11:53:50  138252584 amazon_pqa_chairs.json\r\n",
      "2021-05-09 11:53:50  112165408 amazon_pqa_chargers_&_adapters.json\r\n",
      "2021-05-09 11:53:50  250693003 amazon_pqa_code_readers_&_scan_tools.json\r\n",
      "2021-05-09 11:53:50   68079458 amazon_pqa_computer_cases.json\r\n",
      "2021-05-09 11:53:50   77977101 amazon_pqa_consoles.json\r\n",
      "2021-05-09 11:53:50  197194037 amazon_pqa_costumes.json\r\n",
      "2021-05-09 11:53:50  202777974 amazon_pqa_cradles.json\r\n",
      "2021-05-09 11:53:50  159496851 amazon_pqa_diffusers.json\r\n",
      "2021-05-09 11:53:50   86792506 amazon_pqa_dolls.json\r\n",
      "2021-05-09 11:53:50  319608519 amazon_pqa_dome_cameras.json\r\n",
      "2021-05-09 11:53:50  699110395 amazon_pqa_earbud_headphones.json\r\n",
      "2021-05-09 11:53:50   67925281 amazon_pqa_external_hard_drives.json\r\n",
      "2021-05-09 11:53:50   90896757 amazon_pqa_fashion_sneakers.json\r\n",
      "2021-05-09 11:53:50   98297179 amazon_pqa_floor_mats.json\r\n",
      "2021-05-09 11:53:50  103299169 amazon_pqa_food_storage_&_organization_sets.json\r\n",
      "2021-05-09 11:53:50  241697772 amazon_pqa_games.json\r\n",
      "2021-05-09 11:53:50   77147759 amazon_pqa_graphics_cards.json\r\n",
      "2021-05-09 11:53:50  212819467 amazon_pqa_gun_holsters.json\r\n",
      "2021-05-09 11:53:50  158306869 amazon_pqa_hair_extensions.json\r\n",
      "2021-05-09 11:53:50  138854761 amazon_pqa_handheld_flashlights.json\r\n",
      "2021-05-09 11:53:50  152627057 amazon_pqa_headlight_&_tail_light_conversion_kits.json\r\n",
      "2021-05-09 11:53:50  132272824 amazon_pqa_headlight_assemblies.json\r\n",
      "2021-05-09 11:53:50  131253948 amazon_pqa_headlight_bulbs.json\r\n",
      "2021-05-09 11:53:50  170374172 amazon_pqa_headsets.json\r\n",
      "2021-05-09 11:53:50  160305063 amazon_pqa_hidden_cameras.json\r\n",
      "2021-05-09 11:53:50   95549762 amazon_pqa_home_office_desks.json\r\n",
      "2021-05-09 11:53:50  203914292 amazon_pqa_home_security_systems.json\r\n",
      "2021-05-09 11:53:50   97187448 amazon_pqa_in-dash_dvd_&_video_receivers.json\r\n",
      "2021-05-09 11:53:50  212843244 amazon_pqa_in-dash_navigation.json\r\n",
      "2021-05-09 11:53:50  134353943 amazon_pqa_inkjet_printers.json\r\n",
      "2021-05-09 11:53:50   85768704 amazon_pqa_jeans.json\r\n",
      "2021-05-09 11:53:50  128135277 amazon_pqa_keyboards.json\r\n",
      "2021-05-09 11:53:50  116416000 amazon_pqa_landline_phones.json\r\n",
      "2021-05-09 11:53:50  178273635 amazon_pqa_led_&_lcd_tvs.json\r\n",
      "2021-05-09 11:53:50  300261569 amazon_pqa_led_bulbs.json\r\n",
      "2021-05-09 11:53:50  210506735 amazon_pqa_led_strip_lights.json\r\n",
      "2021-05-09 11:53:51  102910564 amazon_pqa_light_bars.json\r\n",
      "2021-05-09 11:53:51   96573585 amazon_pqa_masks.json\r\n",
      "2021-05-09 11:53:51  152793447 amazon_pqa_mattresses.json\r\n",
      "2021-05-09 11:53:51   94675352 amazon_pqa_memory.json\r\n",
      "2021-05-09 11:53:51  281321641 amazon_pqa_monitors.json\r\n",
      "2021-05-09 11:53:51   78448528 amazon_pqa_motherboards.json\r\n",
      "2021-05-09 11:53:51  150541937 amazon_pqa_mp3_&_mp4_players.json\r\n",
      "2021-05-09 11:53:51  278177567 amazon_pqa_on-dash_cameras.json\r\n",
      "2021-05-09 11:53:51  278531682 amazon_pqa_over-ear_headphones.json\r\n",
      "2021-05-09 11:53:51  147083447 amazon_pqa_panels.json\r\n",
      "2021-05-09 11:53:51   88634925 amazon_pqa_pants.json\r\n",
      "2021-05-09 11:53:51  358988080 amazon_pqa_portable_bluetooth_speakers.json\r\n",
      "2021-05-09 11:53:51   89016682 amazon_pqa_posters_&_prints.json\r\n",
      "2021-05-09 11:53:51  116017016 amazon_pqa_power_converters.json\r\n",
      "2021-05-09 11:53:51  100128300 amazon_pqa_pumps.json\r\n",
      "2021-05-09 11:53:51  172268436 amazon_pqa_quadcopters_&_multirotors.json\r\n",
      "2021-05-09 11:53:51   97803297 amazon_pqa_receivers.json\r\n",
      "2021-05-09 11:53:51  171552858 amazon_pqa_remote_controls.json\r\n",
      "2021-05-09 11:53:51  176770768 amazon_pqa_repellents.json\r\n",
      "2021-05-09 11:53:51  151415809 amazon_pqa_routers.json\r\n",
      "2021-05-09 11:53:51  240899181 amazon_pqa_screen_protectors.json\r\n",
      "2021-05-09 11:53:51  120357793 amazon_pqa_sets.json\r\n",
      "2021-05-09 11:53:51  116292067 amazon_pqa_sheet_&_pillowcase_sets.json\r\n",
      "2021-05-09 11:53:52   88400141 amazon_pqa_slr_camera_lenses.json\r\n",
      "2021-05-09 11:53:52  336801174 amazon_pqa_smartwatches.json\r\n",
      "2021-05-09 11:53:52   83154297 amazon_pqa_sound_bars.json\r\n",
      "2021-05-09 11:53:52  144038367 amazon_pqa_sports_&_action_video_cameras.json\r\n",
      "2021-05-09 11:53:52  132584223 amazon_pqa_sports_water_bottles.json\r\n",
      "2021-05-09 11:53:52  158685791 amazon_pqa_stands.json\r\n",
      "2021-05-09 11:53:52  320663018 amazon_pqa_streaming_media_players.json\r\n",
      "2021-05-09 11:53:52  169509230 amazon_pqa_string_lights.json\r\n",
      "2021-05-09 11:53:52  165270811 amazon_pqa_sunglasses.json\r\n",
      "2021-05-09 11:53:52  216048049 amazon_pqa_surveillance_dvr_kits.json\r\n",
      "2021-05-09 11:53:53  154666084 amazon_pqa_t-shirts.json\r\n",
      "2021-05-09 11:53:52  367319312 amazon_pqa_tablets.json\r\n",
      "2021-05-09 11:53:53  142983066 amazon_pqa_towers.json\r\n",
      "2021-05-09 11:53:53  403702059 amazon_pqa_traditional_laptops.json\r\n",
      "2021-05-09 11:53:53  120175498 amazon_pqa_tv_antennas.json\r\n",
      "2021-05-09 11:53:53  161324981 amazon_pqa_tv_ceiling_&_wall_mounts.json\r\n",
      "2021-05-09 11:53:53  821444488 amazon_pqa_unlocked_cell_phones.json\r\n",
      "2021-05-09 11:53:53  121908382 amazon_pqa_usb_cables.json\r\n",
      "2021-05-09 11:53:53   99049271 amazon_pqa_usb_flash_drives.json\r\n",
      "2021-05-09 11:53:53  119377555 amazon_pqa_vehicle_backup_cameras.json\r\n",
      "2021-05-09 11:53:53  243675719 amazon_pqa_video_projectors.json\r\n",
      "2021-05-09 11:53:53  187965637 amazon_pqa_wigs.json\r\n",
      "2021-05-09 11:53:53  252079672 amazon_pqa_wrist_watches.json\r\n",
      "2021-06-03 12:27:12      10043 readme.txt\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls --no-sign-request s3://amazon-pqa/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b879d1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://amazon-pqa/amazon_pqa_headsets.json to amazon-pqa/amazon_pqa_headsets.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --no-sign-request s3://amazon-pqa/amazon_pqa_headsets.json ./amazon-pqa/amazon_pqa_headsets.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75dc2b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_pqa(file_name,number_rows=1000):\n",
    "    qa_list = []\n",
    "    df = pd.DataFrame(columns=('question', 'answer','label'))\n",
    "    with open(file_name) as f:\n",
    "        i=0\n",
    "        previous_row_data = None\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            df.loc[i] = [data['question_text'],data['answers'][0]['answer_text'],1.0]\n",
    "            i+=1\n",
    "            if previous_row_data is not None:\n",
    "                df.loc[i] = [data['question_text'],previous_row_data['answers'][0]['answer_text'],0.0]\n",
    "            previous_row_data = data\n",
    "            i+=1\n",
    "            if(i == number_rows*2):\n",
    "                break\n",
    "    return df\n",
    "\n",
    "\n",
    "qa_list = load_pqa('amazon-pqa/amazon_pqa_headsets.json',number_rows=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3aa223c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>does this work with cisco ip phone 7942</td>\n",
       "      <td>Use the Plantronics compatibility guide to see...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is this compatible with the cisco ip phone 797...</td>\n",
       "      <td>Don’t know. Call Plantronics</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is this compatible with the cisco ip phone 797...</td>\n",
       "      <td>Use the Plantronics compatibility guide to see...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If i have a polycom vvx, what adapter cable wi...</td>\n",
       "      <td>Hi Gabrielle, what is the model of VVX?</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>If i have a polycom vvx, what adapter cable wi...</td>\n",
       "      <td>Don’t know. Call Plantronics</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>How good is the microphone quality?</td>\n",
       "      <td>I don't wear glasses personally, but the perso...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>is their an attachment that I can use to conne...</td>\n",
       "      <td>it didn't come with one but someone may make a...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>is their an attachment that I can use to conne...</td>\n",
       "      <td>Its actully really good.  when i play with ny ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>Do these have more bass than the game zeros?</td>\n",
       "      <td>Due to the closed back design the Game ZERO wi...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>Do these have more bass than the game zeros?</td>\n",
       "      <td>it didn't come with one but someone may make a...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1999 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "0               does this work with cisco ip phone 7942   \n",
       "2     Is this compatible with the cisco ip phone 797...   \n",
       "3     Is this compatible with the cisco ip phone 797...   \n",
       "4     If i have a polycom vvx, what adapter cable wi...   \n",
       "5     If i have a polycom vvx, what adapter cable wi...   \n",
       "...                                                 ...   \n",
       "1995                How good is the microphone quality?   \n",
       "1996  is their an attachment that I can use to conne...   \n",
       "1997  is their an attachment that I can use to conne...   \n",
       "1998       Do these have more bass than the game zeros?   \n",
       "1999       Do these have more bass than the game zeros?   \n",
       "\n",
       "                                                 answer  label  \n",
       "0     Use the Plantronics compatibility guide to see...    1.0  \n",
       "2                          Don’t know. Call Plantronics    1.0  \n",
       "3     Use the Plantronics compatibility guide to see...    0.0  \n",
       "4               Hi Gabrielle, what is the model of VVX?    1.0  \n",
       "5                          Don’t know. Call Plantronics    0.0  \n",
       "...                                                 ...    ...  \n",
       "1995  I don't wear glasses personally, but the perso...    0.0  \n",
       "1996  it didn't come with one but someone may make a...    1.0  \n",
       "1997  Its actully really good.  when i play with ny ...    0.0  \n",
       "1998  Due to the closed back design the Game ZERO wi...    1.0  \n",
       "1999  it didn't come with one but someone may make a...    0.0  \n",
       "\n",
       "[1999 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a728fed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers.readers import InputExample\n",
    "\n",
    "train_set,test_set = train_test_split(qa_list,test_size=0.2,shuffle=True)\n",
    "training_set, validation_set = train_test_split(train_set,test_size=0.2)\n",
    "\n",
    "def create_input_sample(data_set):\n",
    "    train_samples = []\n",
    "    for index,row in data_set.iterrows():\n",
    "        input_example = InputExample(texts=[row['question'], row['answer']], label=row['label'])\n",
    "        train_samples.append(input_example)\n",
    "    return train_samples\n",
    "\n",
    "training_samples = create_input_sample(training_set)\n",
    "validation_samples = create_input_sample(validation_set)\n",
    "test_samples = create_input_sample(test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7cc4e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f315b5f75f04fa6b7f3defc848de348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b81c3369c8544da98d66271b0c14463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.40803412298270003"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer,  LoggingHandler, losses, models, util\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "\n",
    "model_name = \"sentence-transformers/distilbert-base-nli-stsb-mean-tokens\"\n",
    "train_batch_size = 16\n",
    "num_epochs = 1\n",
    "model_save_path = 'output/fine_tuned_'+model_name.replace(\"/\", \"-\")\n",
    "\n",
    "# Use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for mapping tokens to embeddings\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False)\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "train_dataloader = DataLoader(training_samples, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(validation_samples, name='pqa-valucation')\n",
    "\n",
    "\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs  * 0.1) #10% of train data for warm-up\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path)\n",
    "\n",
    "\n",
    "model = SentenceTransformer(model_save_path)\n",
    "test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, name='pqa-test')\n",
    "test_evaluator(model, output_path=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f3f90da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_Pooling/\n",
      "1_Pooling/config.json\n",
      "config.json\n",
      "config_sentence_transformers.json\n",
      "eval/\n",
      "eval/similarity_evaluation_pqa-valucation_results.csv\n",
      "modules.json\n",
      "pytorch_model.bin\n",
      "README.md\n",
      "sentence_bert_config.json\n",
      "similarity_evaluation_pqa-test_results.csv\n",
      "special_tokens_map.json\n",
      "tokenizer_config.json\n",
      "tokenizer.json\n",
      "vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!cd output/fine_tuned_sentence-transformers-distilbert-base-nli-stsb-mean-tokens && tar czvf ../model.tar.gz *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c0f14cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-522880334446/fine-tuned-transformers-model/model.tar.gz'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "inputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='fine-tuned-transformers-model')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a335ae81",
   "metadata": {},
   "source": [
    "### Deploy the BERT model to SageMaker Endpoint\n",
    "\n",
    "First we need to create a PyTorchModel object. The deploy() method on the model object creates an endpoint which serves prediction requests in real-time. If the instance_type is set to a SageMaker instance type (e.g. ml.m5.large) then the model will be deployed on SageMaker. If the instance_type parameter is set to local then it will be deployed locally as a Docker container and ready for testing locally.\n",
    "\n",
    "First we need to create a Predictor class to accept TEXT as input and output JSON. The default behaviour is to accept a numpy array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6adb61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch, PyTorchModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "class StringPredictor(Predictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text/plain')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20fa68e",
   "metadata": {},
   "source": [
    "Deploy the BERT model to Sagemaker Endpoint\n",
    "\n",
    "#### Note: This process will take serveral minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf81104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data = inputs, \n",
    "                             role=role, \n",
    "                             entry_point ='inference.py',\n",
    "                             source_dir = './code',\n",
    "                             py_version = 'py38', \n",
    "                             framework_version = '1.10.2',\n",
    "                             predictor_cls=StringPredictor)\n",
    "\n",
    "predictor = pytorch_model.deploy(instance_type='ml.m5d.large', \n",
    "                                 initial_instance_count=1, \n",
    "                                 endpoint_name = f'semantic-search-model-{int(time.time())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9413f19",
   "metadata": {},
   "source": [
    "### Test the SageMaker Endpoint.\n",
    "\n",
    "Input is text data, output is vector data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9949550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "original_payload = 'Does this work with xbox?'\n",
    "features = predictor.predict(original_payload)\n",
    "vector_data = json.loads(features)\n",
    "\n",
    "vector_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076f6f96",
   "metadata": {},
   "source": [
    "## Step 2: Ingest data to OpenSearch Cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d33a893",
   "metadata": {},
   "source": [
    "Use Python API to set up connection with OpenSearch Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a4f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "import boto3\n",
    "\n",
    "region = 'us-east-1' \n",
    "\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region)\n",
    "index_name = 'nlp_pqa'\n",
    "\n",
    "aos_client = OpenSearch(\n",
    "    hosts = [{'host': aos_host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87407ed",
   "metadata": {},
   "source": [
    "Create a index with 2 fields, the first field is \"content\" for raw sentece, the second field is \"nlp_article_vector\" for vector data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a006abe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_index = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "        \"index.knn.space_type\": \"cosinesimil\",\n",
    "        \"analysis\": {\n",
    "          \"analyzer\": {\n",
    "            \"default\": {\n",
    "              \"type\": \"standard\",\n",
    "              \"stopwords\": \"_english_\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"question_vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 768,\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"question\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"answer\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8285e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "aos_client.indices.delete(index=\"nlp_pqa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7e7481",
   "metadata": {},
   "outputs": [],
   "source": [
    "aos_client.indices.create(index=\"nlp_pqa\",body=knn_index,ignore=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d54d929",
   "metadata": {},
   "source": [
    "Show the created index information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7425b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "aos_client.indices.get(index=\"nlp_pqa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e987fb",
   "metadata": {},
   "source": [
    "### We can ingest 1000 rows data for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6ac5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "\n",
    "def load_pqa_as_json(file_name,number_rows=1000):\n",
    "    result=[]\n",
    "    with open(file_name) as f:\n",
    "        i=0\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            result.append(data)\n",
    "            i+=1\n",
    "            if(i == number_rows):\n",
    "                break\n",
    "    return result\n",
    "\n",
    "\n",
    "qa_list_json = load_pqa_as_json('amazon-pqa/amazon_pqa_headsets.json',number_rows=1000)\n",
    "\n",
    "\n",
    "def es_import(question):\n",
    "    vector = json.loads(predictor.predict(question[\"question_text\"]))\n",
    "    aos_client.index(index='nlp_pqa',\n",
    "             body={\"question_vector\": vector, \"question\": question[\"question_text\"],\"answer\":question[\"answers\"][0][\"answer_text\"]}\n",
    "            )\n",
    "        \n",
    "workers = 4 * cpu_count()\n",
    "    \n",
    "process_map(es_import, qa_list_json, max_workers=workers,chunksize=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1019e9ca",
   "metadata": {},
   "source": [
    "### Query the documents number in the OpenSearch Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b933fc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = aos_client.search(index=\"nlp_pqa\", body={\"query\": {\"match_all\": {}}})\n",
    "print(\"Got %d Hits:\" % res['hits']['total']['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbc42b6",
   "metadata": {},
   "source": [
    "## Step 3: Semantic Search \n",
    "### Generate vector data for user input query \n",
    "\n",
    "Generate vector data for the question by calling SageMaker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562fbee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_raw_sentences = ['does this work with xbox?']\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "ENDPOINT_NAME = predictor.endpoint\n",
    "response = client.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n",
    "                                       ContentType='text/plain',\n",
    "                                       Body=query_raw_sentences[0])\n",
    "\n",
    "search_vector = json.loads((response['Body'].read()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327be6bc",
   "metadata": {},
   "source": [
    "### Search vector data with \"Semanatic Search\" \n",
    "\n",
    "OpenSearch KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e4b369",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query={\n",
    "    \"size\": 50,\n",
    "    \"query\": {\n",
    "        \"knn\": {\n",
    "            \"question_vector\":{\n",
    "                \"vector\":search_vector,\n",
    "                \"k\":50\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "res = aos_client.search(index=\"nlp_pqa\", \n",
    "                       body=query,\n",
    "                       stored_fields=[\"question\",\"answer\"])\n",
    "#print(\"Got %d Hits:\" % res['hits']['total']['value'])\n",
    "query_result=[]\n",
    "for hit in res['hits']['hits']:\n",
    "    row=[hit['_id'],hit['_score'],hit['fields']['question'][0],hit['fields']['answer'][0]]\n",
    "    query_result.append(row)\n",
    "\n",
    "query_result_df = pd.DataFrame(data=query_result,columns=[\"_id\",\"_score\",\"question\",\"answer\"])\n",
    "display(query_result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd92aa8",
   "metadata": {},
   "source": [
    "### Search the same query with \"Keyword Search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19b7829",
   "metadata": {},
   "outputs": [],
   "source": [
    "query={\n",
    "    \"size\": 50,\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"question\":\"does this work with xbox?\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "res = aos_client.search(index=\"nlp_pqa\", \n",
    "                       body=query,\n",
    "                       stored_fields=[\"question\",\"answer\"])\n",
    "#print(\"Got %d Hits:\" % res['hits']['total']['value'])\n",
    "query_result=[]\n",
    "for hit in res['hits']['hits']:\n",
    "    row=[hit['_id'],hit['_score'],hit['fields']['question'][0],hit['fields']['answer'][0]]\n",
    "    query_result.append(row)\n",
    "\n",
    "query_result_df = pd.DataFrame(data=query_result,columns=[\"_id\",\"_score\",\"question\",\"answer\"])\n",
    "display(query_result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f88fe0",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Make sure that you stop the notebook instance, delete the Amazon SageMaker endpoint and delete the Elasticsearch domain to prevent any additional charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc02ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint\n",
    "predictor.delete_endpoint()\n",
    "\n",
    "# Empty S3 Contents\n",
    "training_bucket_resource = s3_resource.Bucket(bucket)\n",
    "training_bucket_resource.objects.all().delete()\n",
    "\n",
    "hosting_bucket_resource = s3_resource.Bucket(outputs['s3BucketHostingBucketName'])\n",
    "hosting_bucket_resource.objects.all().delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da3def1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
