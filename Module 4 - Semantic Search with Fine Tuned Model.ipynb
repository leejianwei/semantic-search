{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65763bd3",
   "metadata": {},
   "source": [
    "# Semantic Search with Fine Tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228ba4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc92187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "restartkernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e83812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919e28ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers rank_bm25\n",
    "!pip install -q opensearch-py\n",
    "!pip install -q tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ac3446",
   "metadata": {},
   "source": [
    "### Note change \"cloudformation_stack_name\" to the Cloud Formation stack name when you provision your env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10e75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"static-cloudformation-semantic-search\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "\n",
    "bucket = outputs['s3BucketTraining']\n",
    "aos_host = outputs['DomainEndpoint']\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb3f94e",
   "metadata": {},
   "source": [
    "## Step 1: Fine Tune the modal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fb6d55",
   "metadata": {},
   "source": [
    "## Fine Tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca4a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import SentenceTransformer,  LoggingHandler, losses, models, util\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.readers import InputExample\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import gzip\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d654f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --no-sign-request s3://amazon-pqa/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0922a55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --no-sign-request s3://amazon-pqa/amazon_pqa_headsets.json ./amazon-pqa/amazon_pqa_headsets.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84acee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_pqa(file_name,number_rows=1000):\n",
    "    qa_list = []\n",
    "    df = pd.DataFrame(columns=('question', 'answer','label'))\n",
    "    with open(file_name) as f:\n",
    "        i=0\n",
    "        previous_row_data = None\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            df.loc[i] = [data['question_text'],data['answers'][0]['answer_text'],1.0]\n",
    "            i+=1\n",
    "            if previous_row_data is not None:\n",
    "                df.loc[i] = [data['question_text'],previous_row_data['answers'][0]['answer_text'],0.0]\n",
    "            previous_row_data = data\n",
    "            i+=1\n",
    "            if(i == number_rows*2):\n",
    "                break\n",
    "    return df\n",
    "\n",
    "\n",
    "qa_list = load_pqa('amazon-pqa/amazon_pqa_headsets.json',number_rows=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db962c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f4b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers.readers import InputExample\n",
    "\n",
    "train_set,test_set = train_test_split(qa_list,test_size=0.2,shuffle=True)\n",
    "training_set, validation_set = train_test_split(train_set,test_size=0.2)\n",
    "\n",
    "def create_input_sample(data_set):\n",
    "    train_samples = []\n",
    "    for index,row in data_set.iterrows():\n",
    "        input_example = InputExample(texts=[row['question'], row['answer']], label=row['label'])\n",
    "        train_samples.append(input_example)\n",
    "    return train_samples\n",
    "\n",
    "training_samples = create_input_sample(training_set)\n",
    "validation_samples = create_input_sample(validation_set)\n",
    "test_samples = create_input_sample(test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdf585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer,  LoggingHandler, losses, models, util\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "\n",
    "model_name = \"sentence-transformers/distilbert-base-nli-stsb-mean-tokens\"\n",
    "train_batch_size = 16\n",
    "num_epochs = 1\n",
    "model_save_path = 'output/fine_tuned_'+model_name.replace(\"/\", \"-\")\n",
    "\n",
    "# Use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for mapping tokens to embeddings\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False)\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "train_dataloader = DataLoader(training_samples, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(validation_samples, name='pqa-valucation')\n",
    "\n",
    "\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs  * 0.1) #10% of train data for warm-up\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path)\n",
    "\n",
    "\n",
    "model = SentenceTransformer(model_save_path)\n",
    "test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, name='pqa-test')\n",
    "test_evaluator(model, output_path=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eef469",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd output/fine_tuned_sentence-transformers-distilbert-base-nli-stsb-mean-tokens && tar czvf ../model.tar.gz *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf2cae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "inputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='fine-tuned-transformers-model')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a299ba8f",
   "metadata": {},
   "source": [
    "### Deploy the BERT model to SageMaker Endpoint\n",
    "\n",
    "First we need to create a PyTorchModel object. The deploy() method on the model object creates an endpoint which serves prediction requests in real-time. If the instance_type is set to a SageMaker instance type (e.g. ml.m5.large) then the model will be deployed on SageMaker. If the instance_type parameter is set to local then it will be deployed locally as a Docker container and ready for testing locally.\n",
    "\n",
    "First we need to create a Predictor class to accept TEXT as input and output JSON. The default behaviour is to accept a numpy array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b35f760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch, PyTorchModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "class StringPredictor(Predictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text/plain')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa37c352",
   "metadata": {},
   "source": [
    "Deploy the BERT model to Sagemaker Endpoint\n",
    "\n",
    "#### Note: This process will take serveral minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04879344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data = inputs, \n",
    "                             role=role, \n",
    "                             entry_point ='inference.py',\n",
    "                             source_dir = './code',\n",
    "                             py_version = 'py38', \n",
    "                             framework_version = '1.10.2',\n",
    "                             predictor_cls=StringPredictor)\n",
    "\n",
    "predictor = pytorch_model.deploy(instance_type='ml.m5d.large', \n",
    "                                 initial_instance_count=1, \n",
    "                                 endpoint_name = f'semantic-search-model-{int(time.time())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8c81c2",
   "metadata": {},
   "source": [
    "### Test the SageMaker Endpoint.\n",
    "\n",
    "Input is text data, output is vector data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df79ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "original_payload = 'Does this work with xbox?'\n",
    "features = predictor.predict(original_payload)\n",
    "vector_data = json.loads(features)\n",
    "\n",
    "vector_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0875c4f6",
   "metadata": {},
   "source": [
    "## Step 2: Ingest data to OpenSearch Cluster\n",
    "Load data set of Amazon Product Question and Answer data from : https://registry.opendata.aws/amazon-pqa/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb62090",
   "metadata": {},
   "source": [
    "### Downloading Amazon Production Question and Answer Data\n",
    "\n",
    "Datasets: https://registry.opendata.aws/amazon-pqa/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e0073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --no-sign-request s3://amazon-pqa/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdbc4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --no-sign-request s3://amazon-pqa/amazon_pqa_headsets.json ./amazon-pqa/amazon_pqa_headsets.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8226676c",
   "metadata": {},
   "source": [
    "Use Python API to set up connection with OpenSearch Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2021cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from elasticsearch import Elasticsearch, RequestsHttpConnection\n",
    "# from requests_aws4auth import AWS4Auth\n",
    "# region = 'us-east-1' \n",
    "# service = 'es'\n",
    "# credentials = boto3.Session().get_credentials()\n",
    "# awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "\n",
    "# es_client = Elasticsearch(\n",
    "#     hosts = [{'host': aos_host, 'port': 443}],\n",
    "#     http_auth = awsauth,\n",
    "#     use_ssl = True,\n",
    "#     verify_certs = True,\n",
    "#     connection_class = RequestsHttpConnection\n",
    "# )\n",
    "\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "import boto3\n",
    "\n",
    "#es_host = 'search-semanti-domain-7fc1mmzarfpg-vtklyjm33bhijjarsdhbyl7jxq.us-east-1.es.amazonaws.com' \n",
    "region = 'us-east-1' \n",
    "\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region)\n",
    "index_name = 'nlp_pqa'\n",
    "\n",
    "aos_client = OpenSearch(\n",
    "    hosts = [{'host': aos_host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33db12c6",
   "metadata": {},
   "source": [
    "Create a index with 2 fields, the first field is \"content\" for raw sentece, the second field is \"nlp_article_vector\" for vector data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bfb5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_index = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "        \"index.knn.space_type\": \"cosinesimil\",\n",
    "        \"analysis\": {\n",
    "          \"analyzer\": {\n",
    "            \"default\": {\n",
    "              \"type\": \"standard\",\n",
    "              \"stopwords\": \"_english_\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"question_vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 768,\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"question\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"answer\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7648dee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aos_client.indices.delete(index=\"nlp_pqa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cc7ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "aos_client.indices.create(index=\"nlp_pqa\",body=knn_index,ignore=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88b42af",
   "metadata": {},
   "source": [
    "Show the created index information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb4d703",
   "metadata": {},
   "outputs": [],
   "source": [
    "aos_client.indices.get(index=\"nlp_pqa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682a2d65",
   "metadata": {},
   "source": [
    "### We can ingest 1000 rows data for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068eeb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "\n",
    "def load_pqa_as_json(file_name,number_rows=1000):\n",
    "    result=[]\n",
    "    with open(file_name) as f:\n",
    "        i=0\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            result.append(data)\n",
    "            i+=1\n",
    "            if(i == number_rows):\n",
    "                break\n",
    "    return result\n",
    "\n",
    "\n",
    "qa_list_json = load_pqa_as_json('amazon-pqa/amazon_pqa_headsets.json',number_rows=1000)\n",
    "\n",
    "\n",
    "def es_import(question):\n",
    "    vector = json.loads(predictor.predict(question[\"question_text\"]))\n",
    "    aos_client.index(index='nlp_pqa',\n",
    "             body={\"question_vector\": vector, \"question\": question[\"question_text\"],\"answer\":question[\"answers\"][0][\"answer_text\"]}\n",
    "            )\n",
    "        \n",
    "workers = 4 * cpu_count()\n",
    "    \n",
    "process_map(es_import, qa_list_json, max_workers=workers,chunksize=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76724863",
   "metadata": {},
   "source": [
    "### Query the documents number in the OpenSearch Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17799d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = aos_client.search(index=\"nlp_pqa\", body={\"query\": {\"match_all\": {}}})\n",
    "print(\"Got %d Hits:\" % res['hits']['total']['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5222972",
   "metadata": {},
   "source": [
    "## Step 3: Semantic Search \n",
    "### Generate vector data for user input query \n",
    "\n",
    "Generate vector data for the question by calling SageMaker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb3f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_raw_sentences = ['does this work with xbox?']\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "ENDPOINT_NAME = predictor.endpoint\n",
    "response = client.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n",
    "                                       ContentType='text/plain',\n",
    "                                       Body=query_raw_sentences[0])\n",
    "\n",
    "search_vector = json.loads((response['Body'].read()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a53f0d",
   "metadata": {},
   "source": [
    "### Search vector data with \"Semanatic Search\" \n",
    "\n",
    "OpenSearch KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db2f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query={\n",
    "    \"size\": 50,\n",
    "    \"query\": {\n",
    "        \"knn\": {\n",
    "            \"question_vector\":{\n",
    "                \"vector\":search_vector,\n",
    "                \"k\":50\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "res = aos_client.search(index=\"nlp_pqa\", \n",
    "                       body=query,\n",
    "                       stored_fields=[\"question\",\"answer\"])\n",
    "#print(\"Got %d Hits:\" % res['hits']['total']['value'])\n",
    "query_result=[]\n",
    "for hit in res['hits']['hits']:\n",
    "    row=[hit['_id'],hit['_score'],hit['fields']['question'][0],hit['fields']['answer'][0]]\n",
    "    query_result.append(row)\n",
    "\n",
    "query_result_df = pd.DataFrame(data=query_result,columns=[\"_id\",\"_score\",\"question\",\"answer\"])\n",
    "display(query_result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecf10ba",
   "metadata": {},
   "source": [
    "### Search the same query with \"Keyword Search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebe47f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query={\n",
    "    \"size\": 50,\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"question\":\"does this work with xbox?\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "res = aos_client.search(index=\"nlp_pqa\", \n",
    "                       body=query,\n",
    "                       stored_fields=[\"question\",\"answer\"])\n",
    "#print(\"Got %d Hits:\" % res['hits']['total']['value'])\n",
    "query_result=[]\n",
    "for hit in res['hits']['hits']:\n",
    "    row=[hit['_id'],hit['_score'],hit['fields']['question'][0],hit['fields']['answer'][0]]\n",
    "    query_result.append(row)\n",
    "\n",
    "query_result_df = pd.DataFrame(data=query_result,columns=[\"_id\",\"_score\",\"question\",\"answer\"])\n",
    "display(query_result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cba62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
