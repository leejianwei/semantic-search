{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0864b848",
   "metadata": {},
   "source": [
    "# Semantic Search with Amazon OpenSearch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cc9c09",
   "metadata": {},
   "source": [
    "This is quick demo on how to use Amazon OpeSearch develop semantic search application.\n",
    "\n",
    "![word vector](word2vec.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6261b5fb",
   "metadata": {},
   "source": [
    "## Step 1: Prepare BERT Model in SageMaker\n",
    "\n",
    "### Upgrade PyTorch and restart Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1bc3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac6bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "restartkernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29edf64",
   "metadata": {},
   "source": [
    "### Verify PyTorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77f21e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ad8c6c",
   "metadata": {},
   "source": [
    "### Install required libarary, such as HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeb3e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers\n",
    "!pip install -q boto3\n",
    "!pip install -q requests\n",
    "!pip install -q requests-aws4auth\n",
    "!pip install -q opensearch-py\n",
    "!pip install -q tqdm\n",
    "!pip install -q install transformers[torch]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f85440e",
   "metadata": {},
   "source": [
    "### Print SageMaker version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c268ad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\")\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "print(f'SageMaker SDK Version: {sagemaker.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486c9b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"semantic-search\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "\n",
    "bucket = outputs['s3BucketTraining']\n",
    "aos_host = outputs['DomainEndpoint']\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640de2a",
   "metadata": {},
   "source": [
    "Use Hugging Face BERT model to generate vectorization data, every sentence is 768 dimention data.\n",
    "![BERT](nlp_bert.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0d5735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "#model_name = \"distilbert-base-uncased\"\n",
    "#model_name = \"sentence-transformers/msmarco-distilbert-base-dot-prod-v3\"\n",
    "model_name = \"sentence-transformers/distilbert-base-nli-stsb-mean-tokens\"\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "\n",
    "def sentence_to_vector(raw_inputs):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "    model = DistilBertModel.from_pretrained(model_name)\n",
    "    inputs_tokens = tokenizer(raw_inputs, padding=True, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs_tokens)\n",
    "\n",
    "    sentence_embeddings = mean_pooling(outputs, inputs_tokens['attention_mask'])\n",
    "    return sentence_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e24f424",
   "metadata": {},
   "source": [
    "### Save pre-trained BERT model to local and then upload to S3\n",
    "\n",
    "In this section will host the pretrained BERT model into SageMaker Pytorch model server to generate 768x1 dimension fixed length sentence embedding from [sentence-transformers](https://github.com/UKPLab/sentence-transformers) using [HuggingFace Transformers](https://huggingface.co/sentence-transformers/distilbert-base-nli-stsb-mean-tokens). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2931122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "saved_model_dir = 'transformer'\n",
    "os.makedirs(saved_model_dir, exist_ok=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name) \n",
    "\n",
    "tokenizer.save_pretrained(saved_model_dir)\n",
    "model.save_pretrained(saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacc635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51286d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd transformer && tar czvf ../model.tar.gz *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f15e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload the model to S3\n",
    "\n",
    "inputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='sentence-transformers-model')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c66ebf0",
   "metadata": {},
   "source": [
    "### Deploy the BERT model to SageMaker Endpoint\n",
    "\n",
    "First we need to create a PyTorchModel object. The deploy() method on the model object creates an endpoint which serves prediction requests in real-time. If the instance_type is set to a SageMaker instance type (e.g. ml.m5.large) then the model will be deployed on SageMaker. If the instance_type parameter is set to local then it will be deployed locally as a Docker container and ready for testing locally.\n",
    "\n",
    "First we need to create a Predictor class to accept TEXT as input and output JSON. The default behaviour is to accept a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221d81dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch, PyTorchModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "class StringPredictor(Predictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text/plain')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e141e0",
   "metadata": {},
   "source": [
    "Deploy the BERT model to Sagemaker Endpoint\n",
    "\n",
    "#### Note: This process will take serveral minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caddbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model = PyTorchModel(model_data = inputs, \n",
    "                             role=role, \n",
    "                             entry_point ='inference.py',\n",
    "                             source_dir = './code',\n",
    "                             py_version = 'py38', \n",
    "                             framework_version = '1.10.2',\n",
    "                             predictor_cls=StringPredictor)\n",
    "\n",
    "predictor = pytorch_model.deploy(instance_type='ml.g4dn.xlarge', \n",
    "                                 initial_instance_count=1, \n",
    "                                 endpoint_name = f'semantic-search-model-{int(time.time())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bdda19",
   "metadata": {},
   "source": [
    "### Test the SageMaker Endpoint.\n",
    "\n",
    "Input is text data, output is vector data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8ecd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "original_payload = 'Does this work with xbox?'\n",
    "features = predictor.predict(original_payload)\n",
    "vector_data = json.loads(features)\n",
    "\n",
    "vector_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19a3c14",
   "metadata": {},
   "source": [
    "## Step 2: Ingest data to OpenSearch Cluster\n",
    "Load data set of Amazon Product Question and Answer data from : https://registry.opendata.aws/amazon-pqa/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36153b96",
   "metadata": {},
   "source": [
    "### Downloading Amazon Production Question and Answer Data\n",
    "\n",
    "Datasets: https://registry.opendata.aws/amazon-pqa/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78097c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --no-sign-request s3://amazon-pqa/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6041b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --no-sign-request s3://amazon-pqa/amazon_pqa_headsets.json ./amazon-pqa/amazon_pqa_headsets.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73c4a35",
   "metadata": {},
   "source": [
    "### We can ingest 1000 rows data for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1aec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_pqa(file_name,number_rows=1000):\n",
    "    qa_list = []\n",
    "    df = pd.DataFrame(columns=('question', 'answer'))\n",
    "    with open(file_name) as f:\n",
    "        i=0\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            df.loc[i] = [data['question_text'],data['answers'][0]['answer_text']]\n",
    "            i+=1\n",
    "            if(i == number_rows):\n",
    "                break\n",
    "    return df\n",
    "\n",
    "\n",
    "qa_list = load_pqa('amazon-pqa/amazon_pqa_headsets.json',number_rows=1000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5960a9",
   "metadata": {},
   "source": [
    "Convert the text data into vector data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1be24b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_sentences = sentence_to_vector(qa_list[\"question\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1c5f6b",
   "metadata": {},
   "source": [
    "Use Python API to set up connection with OpenSearch Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90e35a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from elasticsearch import Elasticsearch, RequestsHttpConnection\n",
    "# from requests_aws4auth import AWS4Auth\n",
    "# region = 'us-east-1' \n",
    "# service = 'es'\n",
    "# credentials = boto3.Session().get_credentials()\n",
    "# awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "\n",
    "# es_client = Elasticsearch(\n",
    "#     hosts = [{'host': aos_host, 'port': 443}],\n",
    "#     http_auth = awsauth,\n",
    "#     use_ssl = True,\n",
    "#     verify_certs = True,\n",
    "#     connection_class = RequestsHttpConnection\n",
    "# )\n",
    "\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "import boto3\n",
    "\n",
    "#es_host = 'search-semanti-domain-7fc1mmzarfpg-vtklyjm33bhijjarsdhbyl7jxq.us-east-1.es.amazonaws.com' \n",
    "region = 'us-east-1' \n",
    "\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region)\n",
    "index_name = 'nlp_pqa'\n",
    "\n",
    "aos_client = OpenSearch(\n",
    "    hosts = [{'host': aos_host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e57d3",
   "metadata": {},
   "source": [
    "Create a index with 2 fields, the first field is \"content\" for raw sentece, the second field is \"nlp_article_vector\" for vector data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d9b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_index = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "        \"index.knn.space_type\": \"cosinesimil\",\n",
    "        \"analysis\": {\n",
    "          \"analyzer\": {\n",
    "            \"default\": {\n",
    "              \"type\": \"standard\",\n",
    "              \"stopwords\": \"_english_\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"question_vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 768,\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"question\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"answer\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0973b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aos_client.indices.delete(index=\"nlp_pqa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caba4a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aos_client.indices.create(index=\"nlp_pqa\",body=knn_index,ignore=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab71f605",
   "metadata": {},
   "source": [
    "Show the created index information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6647b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "aos_client.indices.get(index=\"nlp_pqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822a8105",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for c in qa_list[\"question\"].tolist():\n",
    "    content=c\n",
    "    vector=vector_sentences[i].tolist()\n",
    "    answer=qa_list[\"answer\"][i]\n",
    "    i+=1\n",
    "    aos_client.index(index='nlp_pqa',body={\"question_vector\": vector, \"question\": content,\"answer\":answer})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb50636f",
   "metadata": {},
   "source": [
    "### Ingest all the headset PQA data into OpenSearch Cluster\n",
    "Comment out the following code to ingest all the headset question, answer and corresponding question vector data into OpenSearch index. \n",
    "\n",
    "### Note: it will take more than 10 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841bb538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from tqdm.contrib.concurrent import process_map\n",
    "# from multiprocessing import cpu_count\n",
    "\n",
    "\n",
    "# def load_pqa_as_json(file_name):\n",
    "#     result=[]\n",
    "#     with open(file_name) as f:\n",
    "#         for line in f:\n",
    "#             data = json.loads(line)\n",
    "#             result.append(data)\n",
    "#     return result\n",
    "\n",
    "\n",
    "# qa_list_json = load_pqa_as_json('amazon-pqa/amazon_pqa_headsets.json')\n",
    "\n",
    "\n",
    "# def es_import(question):\n",
    "#     vector = json.loads(predictor.predict(question[\"question_text\"]))\n",
    "#     aos_client.index(index='nlp_pqa',\n",
    "#              body={\"question_vector\": vector, \"question\": question[\"question_text\"],\"answer\":question[\"answers\"][0][\"answer_text\"]}\n",
    "#             )\n",
    "        \n",
    "# workers = 4 * cpu_count()\n",
    "    \n",
    "# process_map(es_import, qa_list_json, max_workers=workers,chunksize=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ee7128",
   "metadata": {},
   "source": [
    "### Query the documents number in the OpenSearch Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed16ba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = aos_client.search(index=\"nlp_pqa\", body={\"query\": {\"match_all\": {}}})\n",
    "print(\"Got %d Hits:\" % res['hits']['total']['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5e5689",
   "metadata": {},
   "source": [
    "## Step 3: Semantic Search \n",
    "### Generate vector data for user input query \n",
    "\n",
    "Generate vector data for the question by calling SageMaker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0731e3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_raw_sentences = ['does this work with xbox?']\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "ENDPOINT_NAME = predictor.endpoint\n",
    "response = client.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n",
    "                                       ContentType='text/plain',\n",
    "                                       Body=query_raw_sentences[0])\n",
    "\n",
    "search_vector = json.loads((response['Body'].read()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e0e04",
   "metadata": {},
   "source": [
    "### Search vector data with \"Semanatic Search\" \n",
    "\n",
    "OpenSearch KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285d01e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query={\n",
    "    \"size\": 50,\n",
    "    \"query\": {\n",
    "        \"knn\": {\n",
    "            \"question_vector\":{\n",
    "                \"vector\":search_vector,\n",
    "                \"k\":50\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "res = aos_client.search(index=\"nlp_pqa\", \n",
    "                       body=query,\n",
    "                       stored_fields=[\"question\",\"answer\"])\n",
    "#print(\"Got %d Hits:\" % res['hits']['total']['value'])\n",
    "query_result=[]\n",
    "for hit in res['hits']['hits']:\n",
    "    row=[hit['_id'],hit['_score'],hit['fields']['question'][0],hit['fields']['answer'][0]]\n",
    "    query_result.append(row)\n",
    "\n",
    "query_result_df = pd.DataFrame(data=query_result,columns=[\"_id\",\"_score\",\"question\",\"answer\"])\n",
    "display(query_result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a80c7e0",
   "metadata": {},
   "source": [
    "### Search the same query with \"Keyword Search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90fa9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query={\n",
    "    \"size\": 50,\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"question\":\"does this work with xbox?\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "res = aos_client.search(index=\"nlp_pqa\", \n",
    "                       body=query,\n",
    "                       stored_fields=[\"question\",\"answer\"])\n",
    "#print(\"Got %d Hits:\" % res['hits']['total']['value'])\n",
    "query_result=[]\n",
    "for hit in res['hits']['hits']:\n",
    "    row=[hit['_id'],hit['_score'],hit['fields']['question'][0],hit['fields']['answer'][0]]\n",
    "    query_result.append(row)\n",
    "\n",
    "query_result_df = pd.DataFrame(data=query_result,columns=[\"_id\",\"_score\",\"question\",\"answer\"])\n",
    "display(query_result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6840007a",
   "metadata": {},
   "source": [
    "## Step 4: Deploying a full-stack semantic search application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de49213",
   "metadata": {},
   "source": [
    "### Disable S3 \"Block all public access\"\n",
    "\n",
    "Go to S3 Console, click \"Block Public Access settings for this account\" make sure \"Block all public access\" is off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19076cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource.Object(bucket, 'backend/template.yaml').upload_file('./backend/template.yaml', ExtraArgs={'ACL':'public-read'})\n",
    "\n",
    "\n",
    "sam_template_url = f'https://{bucket}.s3.amazonaws.com/backend/template.yaml'\n",
    "print(\"cloudformation template url:\" + sam_template_url)\n",
    "\n",
    "\n",
    "# Generate the CloudFormation Quick Create Link\n",
    "\n",
    "print(\"Click the URL below to create the backend API for NLU search:\\n\")\n",
    "print((\n",
    "    'https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/create/review'\n",
    "    f'?templateURL={sam_template_url}'\n",
    "    '&stackName=semantic-search-api'\n",
    "    f'&param_BucketName={outputs[\"s3BucketTraining\"]}'\n",
    "    f'&param_DomainName={outputs[\"osDomainName\"]}'\n",
    "    f'&param_ElasticSearchURL={outputs[\"DomainEndpoint\"]}'\n",
    "    f'&param_SagemakerEndpoint={predictor.endpoint}'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7eac9b",
   "metadata": {},
   "source": [
    "Now that you have a working Amazon SageMaker endpoint for extracting image features and a KNN index on Elasticsearch, you are ready to build a real-world full-stack ML-powered web app. The SAM template you just created will deploy an Amazon API Gateway and AWS Lambda function. The Lambda function runs your code in response to HTTP requests that are sent to the API Gateway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671e91ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize backend/lambda/app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76486db6",
   "metadata": {},
   "source": [
    "## Once the CloudFormation Stack shows CREATE_COMPLETE, proceed to this cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1952614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "api_endpoint = get_cfn_outputs('semantic-search-api')['TextSimilarityApi']\n",
    "\n",
    "with open('./frontend/src/config/config.json', 'w') as outfile:\n",
    "    json.dump({'apiEndpoint': api_endpoint}, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb025c9",
   "metadata": {},
   "source": [
    "## Deploy frontend services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86529df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add NPM to the path so we can assemble the web frontend from our notebook code\n",
    "\n",
    "from os import environ\n",
    "\n",
    "npm_path = ':/home/ec2-user/anaconda3/envs/JupyterSystemEnv/bin'\n",
    "\n",
    "if npm_path not in environ['PATH']:\n",
    "    ADD_NPM_PATH = environ['PATH']\n",
    "    ADD_NPM_PATH = ADD_NPM_PATH + npm_path\n",
    "else:\n",
    "    ADD_NPM_PATH = environ['PATH']\n",
    "    \n",
    "%set_env PATH=$ADD_NPM_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdced57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./frontend/\n",
    "\n",
    "!npm install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dac308",
   "metadata": {},
   "outputs": [],
   "source": [
    "!npm run-script build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b978d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "hosting_bucket = f\"s3://{outputs['s3BucketHostingBucketName']}\"\n",
    "\n",
    "!aws s3 sync ./build/ $hosting_bucket --acl public-read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30dd396",
   "metadata": {},
   "source": [
    "## Browse your frontend service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ee8d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Click the URL below:\\n')\n",
    "print(outputs['S3BucketSecureURL'] + '/index.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b78a32",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Make sure that you stop the notebook instance, delete the Amazon SageMaker endpoint and delete the Elasticsearch domain to prevent any additional charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff71197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint\n",
    "predictor.delete_endpoint()\n",
    "\n",
    "# Empty S3 Contents\n",
    "training_bucket_resource = s3_resource.Bucket(bucket)\n",
    "training_bucket_resource.objects.all().delete()\n",
    "\n",
    "hosting_bucket_resource = s3_resource.Bucket(outputs['s3BucketHostingBucketName'])\n",
    "hosting_bucket_resource.objects.all().delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa7c269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
