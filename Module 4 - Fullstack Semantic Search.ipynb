{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be3df19b",
   "metadata": {},
   "source": [
    "# Semantic Search with Amazon OpenSearch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c4cc0b",
   "metadata": {},
   "source": [
    "### Upgrade PyTorch and restart Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd82a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bec3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "restartkernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7263546f",
   "metadata": {},
   "source": [
    "### Verify PyTorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a286aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9288676b",
   "metadata": {},
   "source": [
    "### Install required libarary, such as HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81496b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers\n",
    "!pip install -q boto3\n",
    "!pip install -q requests\n",
    "!pip install -q requests-aws4auth\n",
    "!pip install -q opensearch-py\n",
    "!pip install -q tqdm\n",
    "!pip install -q install transformers[torch]\n",
    "!pip install -U sentence-transformers rank_bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003d724c",
   "metadata": {},
   "source": [
    "### Initialize Boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4512c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\")\n",
    "s3 = boto3.client('s3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4762a6c",
   "metadata": {},
   "source": [
    "## Get Cloud Formation stack output variables\n",
    "\n",
    "### Note change \"cloudformation_stack_name\" to the Cloud Formation stack name when you provision your env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3150ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"static-cloudformation-semantic-search\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "\n",
    "bucket = outputs['s3BucketTraining']\n",
    "aos_host = outputs['DomainEndpoint']\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4991f17f",
   "metadata": {},
   "source": [
    "## Step 1: Prepare BERT Model in SageMaker\n",
    "\n",
    "Use Hugging Face BERT model to generate vectorization data, every sentence is 768 dimention data.\n",
    "![BERT](nlp_bert.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b862317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "#model_name = \"distilbert-base-uncased\"\n",
    "#model_name = \"sentence-transformers/msmarco-distilbert-base-dot-prod-v3\"\n",
    "model_name = \"sentence-transformers/distilbert-base-nli-stsb-mean-tokens\"\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "\n",
    "def sentence_to_vector(raw_inputs):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "    model = DistilBertModel.from_pretrained(model_name)\n",
    "    inputs_tokens = tokenizer(raw_inputs, padding=True, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs_tokens)\n",
    "\n",
    "    sentence_embeddings = mean_pooling(outputs, inputs_tokens['attention_mask'])\n",
    "    return sentence_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e000d501",
   "metadata": {},
   "source": [
    "### Save pre-trained BERT model to local and then upload to S3\n",
    "\n",
    "In this section will host the pretrained BERT model into SageMaker Pytorch model server to generate 768x1 dimension fixed length sentence embedding from [sentence-transformers](https://github.com/UKPLab/sentence-transformers) using [HuggingFace Transformers](https://huggingface.co/sentence-transformers/distilbert-base-nli-stsb-mean-tokens). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e91a98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "saved_model_dir = 'transformer'\n",
    "os.makedirs(saved_model_dir, exist_ok=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name) \n",
    "\n",
    "tokenizer.save_pretrained(saved_model_dir)\n",
    "model.save_pretrained(saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211ac9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47f32bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd transformer && tar czvf ../model.tar.gz *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a4bd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload the model to S3\n",
    "\n",
    "inputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='sentence-transformers-model')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475f20aa",
   "metadata": {},
   "source": [
    "### Deploy the BERT model to SageMaker Endpoint\n",
    "\n",
    "First we need to create a PyTorchModel object. The deploy() method on the model object creates an endpoint which serves prediction requests in real-time. If the instance_type is set to a SageMaker instance type (e.g. ml.m5.large) then the model will be deployed on SageMaker. If the instance_type parameter is set to local then it will be deployed locally as a Docker container and ready for testing locally.\n",
    "\n",
    "First we need to create a Predictor class to accept TEXT as input and output JSON. The default behaviour is to accept a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe05bd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch, PyTorchModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "class StringPredictor(Predictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text/plain')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65a56e8",
   "metadata": {},
   "source": [
    "Deploy the BERT model to Sagemaker Endpoint\n",
    "\n",
    "#### Note: This process will take serveral minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd8a86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model = PyTorchModel(model_data = inputs, \n",
    "                             role=role, \n",
    "                             entry_point ='inference.py',\n",
    "                             source_dir = './code',\n",
    "                             py_version = 'py38', \n",
    "                             framework_version = '1.10.2',\n",
    "                             predictor_cls=StringPredictor)\n",
    "\n",
    "predictor = pytorch_model.deploy(instance_type='ml.m5d.large', \n",
    "                                 initial_instance_count=1, \n",
    "                                 endpoint_name = f'semantic-search-model-{int(time.time())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9386c2c7",
   "metadata": {},
   "source": [
    "### Test the SageMaker Endpoint.\n",
    "\n",
    "Input is text data, output is vector data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a261396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "original_payload = 'Does this work with xbox?'\n",
    "features = predictor.predict(original_payload)\n",
    "vector_data = json.loads(features)\n",
    "\n",
    "vector_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7ebaab",
   "metadata": {},
   "source": [
    "## Step 2: Ingest data to OpenSearch Cluster\n",
    "Load data set of Amazon Product Question and Answer data from : https://registry.opendata.aws/amazon-pqa/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09884b3c",
   "metadata": {},
   "source": [
    "### Downloading Amazon Production Question and Answer Data\n",
    "\n",
    "Datasets: https://registry.opendata.aws/amazon-pqa/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f295c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --no-sign-request s3://amazon-pqa/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d732606",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --no-sign-request s3://amazon-pqa/amazon_pqa_headsets.json ./amazon-pqa/amazon_pqa_headsets.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b802b54",
   "metadata": {},
   "source": [
    "### We can ingest 1000 rows data for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a723027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_pqa(file_name,number_rows=1000):\n",
    "    qa_list = []\n",
    "    df = pd.DataFrame(columns=('question', 'answer'))\n",
    "    with open(file_name) as f:\n",
    "        i=0\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            df.loc[i] = [data['question_text'],data['answers'][0]['answer_text']]\n",
    "            i+=1\n",
    "            if(i == number_rows):\n",
    "                break\n",
    "    return df\n",
    "\n",
    "\n",
    "qa_list = load_pqa('amazon-pqa/amazon_pqa_headsets.json',number_rows=1000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dd010d",
   "metadata": {},
   "source": [
    "Convert the text data into vector data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce4272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_sentences = sentence_to_vector(qa_list[\"question\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844cb1f2",
   "metadata": {},
   "source": [
    "Use Python API to set up connection with OpenSearch Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9358151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "import boto3\n",
    "\n",
    "region = 'us-east-1' \n",
    "\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region)\n",
    "index_name = 'nlp_pqa'\n",
    "\n",
    "aos_client = OpenSearch(\n",
    "    hosts = [{'host': aos_host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe62312f",
   "metadata": {},
   "source": [
    "Create a index with 2 fields, the first field is \"content\" for raw sentece, the second field is \"nlp_article_vector\" for vector data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abddd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_index = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "        \"index.knn.space_type\": \"cosinesimil\",\n",
    "        \"analysis\": {\n",
    "          \"analyzer\": {\n",
    "            \"default\": {\n",
    "              \"type\": \"standard\",\n",
    "              \"stopwords\": \"_english_\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"question_vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 768,\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"question\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"answer\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d61d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aos_client.indices.delete(index=\"nlp_pqa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa67fe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "aos_client.indices.create(index=\"nlp_pqa\",body=knn_index,ignore=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9474f6",
   "metadata": {},
   "source": [
    "Show the created index information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada9c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "aos_client.indices.get(index=\"nlp_pqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd7b841",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for c in qa_list[\"question\"].tolist():\n",
    "    content=c\n",
    "    vector=vector_sentences[i].tolist()\n",
    "    answer=qa_list[\"answer\"][i]\n",
    "    i+=1\n",
    "    aos_client.index(index='nlp_pqa',body={\"question_vector\": vector, \"question\": content,\"answer\":answer})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683dcc16",
   "metadata": {},
   "source": [
    "### Ingest all the headset PQA data into OpenSearch Cluster\n",
    "Comment out the following code to ingest all the headset question, answer and corresponding question vector data into OpenSearch index. \n",
    "\n",
    "### Note: it will take more than 10 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a404b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from tqdm.contrib.concurrent import process_map\n",
    "# from multiprocessing import cpu_count\n",
    "\n",
    "\n",
    "# def load_pqa_as_json(file_name):\n",
    "#     result=[]\n",
    "#     with open(file_name) as f:\n",
    "#         for line in f:\n",
    "#             data = json.loads(line)\n",
    "#             result.append(data)\n",
    "#     return result\n",
    "\n",
    "\n",
    "# qa_list_json = load_pqa_as_json('amazon-pqa/amazon_pqa_headsets.json')\n",
    "\n",
    "\n",
    "# def es_import(question):\n",
    "#     vector = json.loads(predictor.predict(question[\"question_text\"]))\n",
    "#     aos_client.index(index='nlp_pqa',\n",
    "#              body={\"question_vector\": vector, \"question\": question[\"question_text\"],\"answer\":question[\"answers\"][0][\"answer_text\"]}\n",
    "#             )\n",
    "        \n",
    "# workers = 4 * cpu_count()\n",
    "    \n",
    "# process_map(es_import, qa_list_json, max_workers=workers,chunksize=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c625fd2e",
   "metadata": {},
   "source": [
    "### Query the documents number in the OpenSearch Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832fd985",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = aos_client.search(index=\"nlp_pqa\", body={\"query\": {\"match_all\": {}}})\n",
    "print(\"Got %d Hits:\" % res['hits']['total']['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599703aa",
   "metadata": {},
   "source": [
    "## Step 3: Deploying a full-stack semantic search application\n",
    "\n",
    "The full stack semantic search applicaiton architecure is as following:\n",
    "\n",
    "![full stack semantic search](semantic_search_fullstack.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679aa3a5",
   "metadata": {},
   "source": [
    "Now that you have a working Amazon SageMaker endpoint for extracting image features and a KNN index on Elasticsearch, you are ready to build a real-world full-stack ML-powered web app. The SAM template you just created will deploy an Amazon API Gateway and AWS Lambda function. The Lambda function runs your code in response to HTTP requests that are sent to the API Gateway."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3802ee3",
   "metadata": {},
   "source": [
    "### Build lambda zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90d321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd backend/lambda\n",
    "!sh build-lambda.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900728a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -l lambda.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c947f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ec2-user/SageMaker/semantic-search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c962f57c",
   "metadata": {},
   "source": [
    "### Disable S3 \"Block all public access\"\n",
    "\n",
    "Go to S3 Console, click \"Block Public Access settings for this account\" make sure \"Block all public access\" is off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076aad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource.Object(bucket, 'lambda/lambda.zip').upload_file('./backend/lambda/lambda.zip',ExtraArgs={'ACL':'public-read'})\n",
    "lambda_zip_url = f'{bucket}'\n",
    "print(\"lambada zip file url: \" + lambda_zip_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d17ebc2",
   "metadata": {},
   "source": [
    "### Upload CloudFormation template to create API Gateway and Lambda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33b05e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource.Object(bucket, 'backend/template.yaml').upload_file('./backend/template.yaml', ExtraArgs={'ACL':'public-read'})\n",
    "\n",
    "\n",
    "sam_template_url = f'https://{bucket}.s3.amazonaws.com/backend/template.yaml'\n",
    "print(\"cloudformation template url:\" + sam_template_url)\n",
    "\n",
    "\n",
    "# Generate the CloudFormation Quick Create Link\n",
    "\n",
    "print(\"Click the URL below to create the backend API for NLU search:\\n\")\n",
    "print((\n",
    "    'https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/create/review'\n",
    "    f'?templateURL={sam_template_url}'\n",
    "    '&stackName=semantic-search-api'\n",
    "    f'&param_BucketName={outputs[\"s3BucketTraining\"]}'\n",
    "    f'&param_DomainName={outputs[\"osDomainName\"]}'\n",
    "    f'&param_ElasticSearchURL={outputs[\"DomainEndpoint\"]}'\n",
    "    f'&param_SagemakerEndpoint={predictor.endpoint}'\n",
    "    f'&param_LambdaZipFile={lambda_zip_url}'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7376a947",
   "metadata": {},
   "source": [
    "## Once the CloudFormation Stack shows CREATE_COMPLETE, proceed to this cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8a728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "api_endpoint = get_cfn_outputs('semantic-search-api')['TextSimilarityApi']\n",
    "\n",
    "with open('./frontend/src/config/config.json', 'w') as outfile:\n",
    "    json.dump({'apiEndpoint': api_endpoint}, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840473f2",
   "metadata": {},
   "source": [
    "## Deploy frontend services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522ea0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add NPM to the path so we can assemble the web frontend from our notebook code\n",
    "\n",
    "from os import environ\n",
    "\n",
    "npm_path = ':/home/ec2-user/anaconda3/envs/JupyterSystemEnv/bin'\n",
    "\n",
    "if npm_path not in environ['PATH']:\n",
    "    ADD_NPM_PATH = environ['PATH']\n",
    "    ADD_NPM_PATH = ADD_NPM_PATH + npm_path\n",
    "else:\n",
    "    ADD_NPM_PATH = environ['PATH']\n",
    "    \n",
    "%set_env PATH=$ADD_NPM_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509dd380",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./frontend/\n",
    "\n",
    "!npm install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61467f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "!npm run-script build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206c558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hosting_bucket = f\"s3://{outputs['s3BucketHostingBucketName']}\"\n",
    "\n",
    "!aws s3 sync ./build/ $hosting_bucket --acl public-read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9b6b6e",
   "metadata": {},
   "source": [
    "## Browse your frontend service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dcdb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Click the URL below:\\n')\n",
    "print(outputs['S3BucketSecureURL'] + '/index.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfb8614",
   "metadata": {},
   "source": [
    "You can search the question, for example \"does this work with xbox?\", compare the search result. you will see the difference between keyword search and semantic search.\n",
    "\n",
    "![full stack semantic search](full-stack-semantic-search-ui.jpg)\n",
    "\n",
    "In keyword search, some questions like \"Does this work for a switch?\", \"does this work with pc\" which include \"does this work\" are searched however the meaning is totally different with query.\n",
    "\n",
    "In semantic search, some questions like \"Do I need to buy anything extra to used in xbox one s controller?\", \"How do these headphones connect to the Xbox360 controller?\" are searched. The meaning is very close to the query.\n",
    "![full stack semantic search](full-stack-semantic-search-ui-2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5974528",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Make sure that you stop the notebook instance, delete the Amazon SageMaker endpoint and delete the Elasticsearch domain to prevent any additional charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69f1412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint\n",
    "predictor.delete_endpoint()\n",
    "\n",
    "# Empty S3 Contents\n",
    "training_bucket_resource = s3_resource.Bucket(bucket)\n",
    "training_bucket_resource.objects.all().delete()\n",
    "\n",
    "hosting_bucket_resource = s3_resource.Bucket(outputs['s3BucketHostingBucketName'])\n",
    "hosting_bucket_resource.objects.all().delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8f3991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
